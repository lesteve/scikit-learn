
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/ensemble/plot_monotonic_constraints.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_ensemble_plot_monotonic_constraints.py>`
        to download the full example code or to run this example in your browser via JupyterLite or Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_ensemble_plot_monotonic_constraints.py:


=====================
Monotonic Constraints
=====================

This example illustrates the effect of monotonic constraints on a gradient
boosting estimator.

We build an artificial dataset where the target value is in general
positively correlated with the first feature (with some random and
non-random variations), and in general negatively correlated with the second
feature.

By imposing a monotonic increase or a monotonic decrease constraint, respectively,
on the features during the learning process, the estimator is able to properly follow
the general trend instead of being subject to the variations.

This example was inspired by the `XGBoost documentation
<https://xgboost.readthedocs.io/en/latest/tutorials/monotonic.html>`_.

.. GENERATED FROM PYTHON SOURCE LINES 23-41

.. code-block:: default

    from sklearn.ensemble import HistGradientBoostingRegressor
    from sklearn.inspection import PartialDependenceDisplay
    import numpy as np
    import matplotlib.pyplot as plt


    rng = np.random.RandomState(0)

    n_samples = 1000
    f_0 = rng.rand(n_samples)
    f_1 = rng.rand(n_samples)
    X = np.c_[f_0, f_1]
    noise = rng.normal(loc=0.0, scale=0.01, size=n_samples)

    # y is positively correlated with f_0, and negatively correlated with f_1
    y = 5 * f_0 + np.sin(10 * np.pi * f_0) - 5 * f_1 - np.cos(10 * np.pi * f_1) + noise



.. GENERATED FROM PYTHON SOURCE LINES 42-43

Fit a first model on this dataset without any constraints.

.. GENERATED FROM PYTHON SOURCE LINES 43-46

.. code-block:: default

    gbdt_no_cst = HistGradientBoostingRegressor()
    gbdt_no_cst.fit(X, y)


.. GENERATED FROM PYTHON SOURCE LINES 47-49

Fit a second model on this dataset with monotonic increase (1)
and a monotonic decrease (-1) constraints, respectively.

.. GENERATED FROM PYTHON SOURCE LINES 49-53

.. code-block:: default

    gbdt_with_monotonic_cst = HistGradientBoostingRegressor(monotonic_cst=[1, -1])
    gbdt_with_monotonic_cst.fit(X, y)



.. GENERATED FROM PYTHON SOURCE LINES 54-55

Let's display the partial dependence of the predictions on the two features.

.. GENERATED FROM PYTHON SOURCE LINES 55-85

.. code-block:: default

    fig, ax = plt.subplots()
    disp = PartialDependenceDisplay.from_estimator(
        gbdt_no_cst,
        X,
        features=[0, 1],
        feature_names=(
            "First feature",
            "Second feature",
        ),
        line_kw={"linewidth": 4, "label": "unconstrained", "color": "tab:blue"},
        ax=ax,
    )
    PartialDependenceDisplay.from_estimator(
        gbdt_with_monotonic_cst,
        X,
        features=[0, 1],
        line_kw={"linewidth": 4, "label": "constrained", "color": "tab:orange"},
        ax=disp.axes_,
    )

    for f_idx in (0, 1):
        disp.axes_[0, f_idx].plot(
            X[:, f_idx], y, "o", alpha=0.3, zorder=-1, color="tab:green"
        )
        disp.axes_[0, f_idx].set_ylim(-6, 6)

    plt.legend()
    fig.suptitle("Monotonic constraints effect on partial dependences")
    plt.show()


.. GENERATED FROM PYTHON SOURCE LINES 86-89

We can see that the predictions of the unconstrained model capture the
oscillations of the data while the constrained model follows the general
trend and ignores the local variations.

.. GENERATED FROM PYTHON SOURCE LINES 91-98

.. _monotonic_cst_features_names:

Using feature names to specify monotonic constraints
----------------------------------------------------

Note that if the training data has feature names, it's possible to specifiy the
monotonic constraints by passing a dictionary:

.. GENERATED FROM PYTHON SOURCE LINES 98-109

.. code-block:: default

    import pandas as pd

    X_df = pd.DataFrame(X, columns=["f_0", "f_1"])

    gbdt_with_monotonic_cst_df = HistGradientBoostingRegressor(
        monotonic_cst={"f_0": 1, "f_1": -1}
    ).fit(X_df, y)

    np.allclose(
        gbdt_with_monotonic_cst_df.predict(X_df), gbdt_with_monotonic_cst.predict(X)
    )


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.000 seconds)


.. _sphx_glr_download_auto_examples_ensemble_plot_monotonic_constraints.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: binder-badge

      .. image:: images/binder_badge_logo.svg
        :target: https://mybinder.org/v2/gh/scikit-learn/scikit-learn/main?urlpath=lab/tree/notebooks/auto_examples/ensemble/plot_monotonic_constraints.ipynb
        :alt: Launch binder
        :width: 150 px



    .. container:: lite-badge

      .. image:: images/jupyterlite_badge_logo.svg
        :target: ../lite/lab/?path=auto_examples/ensemble/plot_monotonic_constraints.ipynb
        :alt: Launch JupyterLite
        :width: 150 px

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_monotonic_constraints.py <plot_monotonic_constraints.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_monotonic_constraints.ipynb <plot_monotonic_constraints.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
